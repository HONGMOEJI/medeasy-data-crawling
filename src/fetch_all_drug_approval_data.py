import os
import requests
import json
import xml.etree.ElementTree as ET
import re
import html
import time
from dotenv import load_dotenv
from urllib.parse import urlencode

# í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ ê²½ë¡œ
dotenv_path = os.path.join(os.path.dirname(__file__), "..", "configs", ".env")

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(dotenv_path)

# API ê¸°ë³¸ ì •ë³´
BASE_URL = "http://apis.data.go.kr/1471000/DrugPrdtPrmsnInfoService06/getDrugPrdtPrmsnDtlInq05"
API_KEY = os.getenv("DATA_PORTAL_API_KEY_DECODED")  # .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ

# ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
RAW_OUTPUT_FILE = os.path.join(ROOT_DIR, "data", "raw", "drug_approval_data.json")
PROCESSED_OUTPUT_FILE = os.path.join(ROOT_DIR, "data", "processed", "drug_approval_data_processed.json")

# í•„ìš”í•œ í•„ë“œ ëª©ë¡
REQUIRED_FIELDS = [
    "ITEM_SEQ", "ITEM_NAME", "ENTP_NAME", "ETC_OTC_CODE", "ETC_OTC_NAME", "CHART", 
    "EE_DOC_DATA", "UD_DOC_DATA", "NB_DOC_DATA", "STORAGE_METHOD", "VALID_TERM", "CANCEL_DATE"
]

def parse_xml_doc(xml_string):
    """
    CDATA ë‚´ì˜ XML ë¬¸ì„œë¥¼ íŒŒì‹±í•˜ì—¬ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    if not xml_string:
        return None
    
    try:
        # 1ë‹¨ê³„: XMLì„ íŒŒì‹±í•˜ê¸° ì „ì— ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” ë¶€ë¶„ì„ ì •ë¦¬
        
        # HTML íƒœê·¸ë¥¼ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬ (CDATA ë¸”ë¡ ë‚´ë¶€ í¬í•¨)
        xml_string = re.sub(r'<sub>(.*?)</sub>', r'\1', xml_string)
        xml_string = re.sub(r'<sup>(.*?)</sup>', r'\1', xml_string)
        
        # CDATA ë‚´ì˜ &amp; ì²˜ë¦¬ - ì´ë¯¸ &amp;ë¡œ ì¸ì½”ë”©ëœ ê²ƒì€ ê±´ë„ˆë›°ê³  ì¼ë°˜ &ë§Œ ë³€í™˜
        # CDATA ë¸”ë¡ì„ ì„ì‹œë¡œ ì¶”ì¶œ
        cdata_blocks = re.findall(r'<!\[CDATA\[(.*?)\]\]>', xml_string, re.DOTALL)
        
        # ê° CDATA ë¸”ë¡ ë‚´ì—ì„œ ì—”í‹°í‹° ì²˜ë¦¬
        for block in cdata_blocks:
            fixed_block = re.sub(r'&(?!(amp;|lt;|gt;|apos;|quot;|#\d+;|#x[0-9a-fA-F]+;))', '&amp;', block)
            # ì›ë³¸ ë¸”ë¡ì„ ìˆ˜ì •ëœ ë¸”ë¡ìœ¼ë¡œ ëŒ€ì²´
            xml_string = xml_string.replace('<![CDATA[' + block + ']]>', '<![CDATA[' + fixed_block + ']]>')
        
        # 2ë‹¨ê³„: XML íŒŒì‹± ì‹œë„
        try:
            root = ET.fromstring(xml_string)
            
            # ë¬¸ì„œ ì œëª©ê³¼ íƒ€ì… ì¶”ì¶œ
            doc_title = root.get('title', '')
            doc_type = root.get('type', '')
            
            # ê²°ê³¼ êµ¬ì¡° ì´ˆê¸°í™”
            result = {
                'title': doc_title,
                'type': doc_type,
                'sections': []
            }
            
            # ê° ì„¹ì…˜ ì²˜ë¦¬
            for section in root.findall('SECTION'):
                section_data = {
                    'title': section.get('title', ''),
                    'articles': []
                }
                
                # ì„¹ì…˜ ë‚´ ê° ì•„í‹°í´ ì²˜ë¦¬
                for article in section.findall('ARTICLE'):
                    article_data = {
                        'title': article.get('title', ''),
                        'paragraphs': []
                    }
                    
                    # ì•„í‹°í´ ë‚´ ê° ë¬¸ë‹¨ ì²˜ë¦¬
                    for paragraph in article.findall('PARAGRAPH'):
                        # í…ìŠ¤íŠ¸ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸° - textê°€ Noneì´ë©´ ë¹ˆ ë¬¸ìì—´ ì‚¬ìš©
                        text = paragraph.text or ''
                        
                        # CDATA ì²˜ë¦¬
                        if '![CDATA[' in text:
                            # CDATA ë§ˆì»¤ ì œê±°
                            text = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', text, flags=re.DOTALL)
                        
                        # HTML íƒœê·¸ ì œê±° - ë‚¨ì•„ìˆì„ ìˆ˜ ìˆëŠ” ëª¨ë“  HTML íƒœê·¸
                        text = re.sub(r'<[^>]+>', '', text)
                        
                        # HTML ì—”í‹°í‹° ë””ì½”ë”© (ì˜ˆ: &nbsp; -> ê³µë°±)
                        text = html.unescape(text)
                        
                        # ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì ë° íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬
                        text = text.replace('\r', '').replace('\t', ' ')
                        
                        # ì—°ì†ëœ ê³µë°± í•˜ë‚˜ë¡œ ì¹˜í™˜
                        text = re.sub(r' +', ' ', text)
                        
                        # ë¬¸ë‹¨ ì•ë’¤ ê³µë°± ì œê±°
                        text = text.strip()
                        
                        # ë¹„ì–´ìˆì§€ ì•Šì€ ë¬¸ë‹¨ë§Œ ì¶”ê°€
                        if text:
                            article_data['paragraphs'].append(text)
                    
                    # ë‚´ìš©ì´ ìˆëŠ” ì•„í‹°í´ë§Œ ì¶”ê°€
                    if article_data['paragraphs'] or article_data['title']:
                        section_data['articles'].append(article_data)
                
                # ë‚´ìš©ì´ ìˆëŠ” ì„¹ì…˜ë§Œ ì¶”ê°€
                if section_data['articles']:
                    result['sections'].append(section_data)
            
            # í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            text_content = extract_text_from_parsed_doc(result)
            result['text'] = text_content
            
            return result
            
        except ET.ParseError as xml_error:
            # ê³ ê¸‰ ë³µêµ¬ ì‹œë„: ë” ê°•ë ¥í•œ ì „ì²˜ë¦¬ ì ìš©
            print(f"ê¸°ë³¸ XML íŒŒì‹± ì‹¤íŒ¨, ê³ ê¸‰ ë³µêµ¬ ì‹œë„: {xml_error}")
            return handle_complex_xml_parsing(xml_string)
            
    except Exception as e:
        print(f"XML ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        return {
            'title': 'ì²˜ë¦¬ ì˜¤ë¥˜',
            'type': 'error',
            'error': str(e),
            'text': f"ì²˜ë¦¬ ì˜¤ë¥˜: {e}",
            'raw': xml_string[:500] + ('...' if len(xml_string) > 500 else '')
        }

def handle_complex_xml_parsing(xml_string):
    """
    ë” ê°•ë ¥í•œ XML íŒŒì‹± ë³µêµ¬ ë¡œì§ì„ ì ìš©í•©ë‹ˆë‹¤.
    """
    try:
        # CDATA ë§ˆì»¤ ì œê±° (ëª¨ë“  CDATA íƒœê·¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜)
        xml_string = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', xml_string, flags=re.DOTALL)
        
        # HTML íƒœê·¸ë¥¼ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        xml_string = re.sub(r'<sub>(.*?)</sub>', r'\1', xml_string)
        xml_string = re.sub(r'<sup>(.*?)</sup>', r'\1', xml_string)
        xml_string = re.sub(r'<br\s*/?>', ' ', xml_string)
        
        # ëª¨ë“  & ê¸°í˜¸ë¥¼ &amp;ë¡œ ë³€í™˜ (ì´ë¯¸ ì¸ì½”ë”©ëœ ì—”í‹°í‹°ëŠ” ì œì™¸)
        xml_string = re.sub(r'&(?!(amp;|lt;|gt;|apos;|quot;|#\d+;|#x[0-9a-fA-F]+;))', '&amp;', xml_string)
        
        # ë‹«íˆì§€ ì•Šì€ íƒœê·¸ ì²˜ë¦¬
        # ì˜ˆ: <PARAGRAPH> ... (ë‹«ëŠ” íƒœê·¸ ì—†ìŒ) -> <PARAGRAPH> ... </PARAGRAPH>
        opened_tags = re.findall(r'<(\w+)(?:\s+[^>]*)?>[^<]*$', xml_string)
        for tag in reversed(opened_tags):
            xml_string += f'</{tag}>'
        
        # ì´ì œ XML íŒŒì‹± ë‹¤ì‹œ ì‹œë„
        try:
            root = ET.fromstring(xml_string)
            # ì´í›„ ë¡œì§ì€ ê¸°ì¡´ parse_xml_docê³¼ ë™ì¼...
            
            # ë¬¸ì„œ ì œëª©ê³¼ íƒ€ì… ì¶”ì¶œ
            doc_title = root.get('title', '')
            doc_type = root.get('type', '')
            
            # ê²°ê³¼ êµ¬ì¡° ì´ˆê¸°í™”
            result = {
                'title': doc_title,
                'type': doc_type,
                'sections': []
            }
            
            # ê° ì„¹ì…˜ ì²˜ë¦¬
            for section in root.findall('SECTION'):
                section_data = {
                    'title': section.get('title', ''),
                    'articles': []
                }
                
                # ì„¹ì…˜ ë‚´ ê° ì•„í‹°í´ ì²˜ë¦¬
                for article in section.findall('ARTICLE'):
                    article_data = {
                        'title': article.get('title', ''),
                        'paragraphs': []
                    }
                    
                    # ì•„í‹°í´ ë‚´ ê° ë¬¸ë‹¨ ì²˜ë¦¬
                    for paragraph in article.findall('PARAGRAPH'):
                        # í…ìŠ¤íŠ¸ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°
                        text = paragraph.text or ''
                        
                        # HTML íƒœê·¸ ì œê±°
                        text = re.sub(r'<[^>]+>', '', text)
                        
                        # HTML ì—”í‹°í‹° ë””ì½”ë”©
                        text = html.unescape(text)
                        
                        # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬
                        text = text.replace('\r', '').replace('\t', ' ')
                        text = re.sub(r' +', ' ', text)
                        text = text.strip()
                        
                        # ë¹„ì–´ìˆì§€ ì•Šì€ ë¬¸ë‹¨ë§Œ ì¶”ê°€
                        if text:
                            article_data['paragraphs'].append(text)
                    
                    # ë‚´ìš©ì´ ìˆëŠ” ì•„í‹°í´ë§Œ ì¶”ê°€
                    if article_data['paragraphs'] or article_data['title']:
                        section_data['articles'].append(article_data)
                
                # ë‚´ìš©ì´ ìˆëŠ” ì„¹ì…˜ë§Œ ì¶”ê°€
                if section_data['articles']:
                    result['sections'].append(section_data)
            
            # í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            text_content = extract_text_from_parsed_doc(result)
            result['text'] = text_content
            
            return result
            
        except ET.ParseError:
            # ìµœì¢… ëŒ€ì•ˆ: ë¹„ì •ê·œì‹ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
            return extract_text_from_broken_xml(xml_string)
            
    except Exception as e:
        print(f"ê³ ê¸‰ XML ë³µêµ¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return extract_text_from_broken_xml(xml_string)

def extract_text_from_parsed_doc(doc_data):
    """íŒŒì‹±ëœ ë¬¸ì„œ êµ¬ì¡°ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    result = []
    
    # ë¬¸ì„œ ì œëª© ì¶”ê°€
    if doc_data.get('title'):
        result.append(f"ã€{doc_data['title']}ã€‘")
    
    # ê° ì„¹ì…˜ ì²˜ë¦¬
    for section in doc_data.get('sections', []):
        # ê° ì•„í‹°í´ ì²˜ë¦¬
        for article in section.get('articles', []):
            # ì•„í‹°í´ ì œëª© ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
            if article.get('title'):
                result.append(f"\nâ–  {article['title']}")
            
            # ë¬¸ë‹¨ ì¶”ê°€
            for paragraph in article.get('paragraphs', []):
                result.append(f"- {paragraph}")
    
    return '\n'.join(result)

def extract_text_from_broken_xml(xml_string):
    """
    ì™„ì „íˆ ê¹¨ì§„ XMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œì„ ì‹œë„í•˜ëŠ” í•¨ìˆ˜
    """
    # ì›ë³¸ XML ë°±ì—…
    original_xml = xml_string
    
    try:
        # ëª¨ë“  HTML/XML íƒœê·¸ ì²˜ë¦¬ ì „ CDATA ë§ˆì»¤ ì œê±°
        xml_string = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', xml_string, flags=re.DOTALL)
        
        # HTML íƒœê·¸ ì²˜ë¦¬
        xml_string = re.sub(r'<sub>(.*?)</sub>', r'\1', xml_string)
        xml_string = re.sub(r'<sup>(.*?)</sup>', r'\1', xml_string)
        xml_string = re.sub(r'<br\s*/?>', ' ', xml_string)
        
        # ë¬¸ì„œ ì œëª© ì¶”ì¶œ ì‹œë„
        title_match = re.search(r'title="([^"]+)"', xml_string)
        doc_title = title_match.group(1) if title_match else "ì œëª© ì—†ìŒ"
        
        # SECTION, ARTICLE, PARAGRAPH íƒœê·¸ ì‚¬ì´ì˜ ì½˜í…ì¸  ì¶”ì¶œ
        sections = []
        
        # ì•„í‹°í´ ì œëª© ì¶”ì¶œ
        article_titles = re.findall(r'<ARTICLE title="([^"]+)"', xml_string)
        
        # PARAGRAPH ë‚´ìš© ì¶”ì¶œ
        paragraph_contents = re.findall(r'<PARAGRAPH[^>]*>(.*?)</PARAGRAPH>', xml_string, re.DOTALL)
        
        # êµ¬ì¡°í™”ëœ ê²°ê³¼ ìƒì„±
        result = {
            'title': doc_title,
            'type': 'text_extraction',
            'text': ''
        }
        
        # í…ìŠ¤íŠ¸ êµ¬ì„±
        text_parts = []
        text_parts.append(f"ã€{doc_title}ã€‘")
        
        # ì•„í‹°í´ ì œëª© ì¶”ê°€
        for title in article_titles:
            if title.strip():
                text_parts.append(f"\nâ–  {title}")
        
        # ë¬¸ë‹¨ ë‚´ìš© ì¶”ê°€
        for content in paragraph_contents:
            # HTML íƒœê·¸ ì œê±°
            content = re.sub(r'<[^>]+>', '', content)
            # HTML ì—”í‹°í‹° ë””ì½”ë”©
            content = html.unescape(content)
            # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬
            content = content.replace('\r', '').replace('\t', ' ')
            content = re.sub(r' +', ' ', content)
            content = content.strip()
            
            if content:
                text_parts.append(f"- {content}")
        
        # í…ìŠ¤íŠ¸ ì¡°í•©
        result['text'] = '\n'.join(text_parts)
        
        # í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ì¶”ì¶œ ì‹œë„
        if result['text'] == f"ã€{doc_title}ã€‘":
            # ëª¨ë“  XML íƒœê·¸ ì œê±°
            plain_text = re.sub(r'<[^>]*>', ' ', xml_string)
            plain_text = html.unescape(plain_text)
            plain_text = re.sub(r'\s+', ' ', plain_text).strip()
            
            # ì˜ë¯¸ ìˆëŠ” ë¬¸ì¥ ì¶”ì¶œ
            sentences = re.split(r'[.!?]\s+', plain_text)
            meaningful_sentences = [s.strip() + '.' for s in sentences if len(s.strip()) > 10]
            
            if meaningful_sentences:
                result['text'] = f"ã€{doc_title}ã€‘\n\n" + '\n- '.join([''] + meaningful_sentences)
            else:
                result['text'] = f"ã€{doc_title}ã€‘\n\n- í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨"
        
        return result
        
    except Exception as e:
        print(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ìµœí›„ì˜ ë°©ë²•: ëª¨ë“  XML íƒœê·¸ë¥¼ ì œê±°í•˜ê³  ë‹¨ìˆœ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
        try:
            text = re.sub(r'<[^>]*>', ' ', original_xml)
            text = html.unescape(text)
            text = re.sub(r'\s+', ' ', text).strip()
            
            return {
                'title': 'í…ìŠ¤íŠ¸ ì¶”ì¶œ',
                'type': 'raw_text',
                'text': text
            }
        except:
            return {
                'title': 'ì¶”ì¶œ ì‹¤íŒ¨',
                'type': 'error',
                'text': 'í…ìŠ¤íŠ¸ ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.'
            }

def fetch_drug_approval_data():
    """
    ì˜ì•½í’ˆ í—ˆê°€ ì •ë³´ ë°ì´í„°ë¥¼ APIì—ì„œ ê°€ì ¸ì˜¤ê³  XML ë¬¸ì„œë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤.
    í—ˆê°€ ì·¨ì†Œëœ ì˜ì•½í’ˆê³¼ ìˆ˜ì¶œìš© ì˜ì•½í’ˆì„ ê±´ë„ˆëœë‹ˆë‹¤.
    """
    page_no = 1
    page_size = 100
    total_data = []
    max_retries = 3  # API ìš”ì²­ ì‹¤íŒ¨ ì‹œ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
    last_page_items = -1  # ì´ì „ í˜ì´ì§€ì˜ í•­ëª© ìˆ˜ (ì´ˆê¸°ê°’ì€ -1ë¡œ ì„¤ì •)
    seen_item_sequences = set()  # ì´ë¯¸ ìˆ˜ì§‘í•œ ì•„ì´í…œ ì¼ë ¨ë²ˆí˜¸ ì¶”ì 
    
    # í•„í„°ë§ ì¹´ìš´í„° ì´ˆê¸°í™”
    filtered_canceled_count = 0
    filtered_export_count = 0

    while True:
        params = {
            "serviceKey": API_KEY,
            "pageNo": page_no,
            "numOfRows": page_size,
            "type": "json"
        }

        # ìš”ì²­ URL ìƒì„±
        request_url = f"{BASE_URL}?{urlencode(params)}"
        print(f"ğŸ”— ìš”ì²­ URL: {request_url}")

        retries = 0
        success = False
        
        while retries < max_retries and not success:
            try:
                response = requests.get(request_url, timeout=30)
                print(f"ğŸ” API ì‘ë‹µ ì½”ë“œ: {response.status_code}")
                
                if response.status_code != 200:
                    print(f"âŒ API ìš”ì²­ ì‹¤íŒ¨! ìƒíƒœ ì½”ë“œ: {response.status_code}")
                    print(f"âš ï¸ ì‘ë‹µ í…ìŠ¤íŠ¸: {response.text}")
                    retries += 1
                    if retries < max_retries:
                        print(f"ì¬ì‹œë„ ì¤‘... ({retries}/{max_retries})")
                        time.sleep(2)
                        continue
                    break
                
                # ì‘ë‹µ í™•ì¸ì„ ìœ„í•œ ë¯¸ë¦¬ë³´ê¸°
                response_preview = response.text[:200] + "..." if len(response.text) > 200 else response.text
                print(f"ğŸ” ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {response_preview}")

                # ì‘ë‹µ ë°ì´í„° íŒŒì‹±
                try:
                    data = response.json()
                except json.JSONDecodeError:
                    print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜! ì‘ë‹µì´ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
                    print(f"ì‘ë‹µ ë‚´ìš©: {response.text[:500]}...")
                    retries += 1
                    if retries < max_retries:
                        time.sleep(2)
                        continue
                    break
                
                # ì‘ë‹µ êµ¬ì¡° í™•ì¸
                print(f"ğŸ” ì‘ë‹µ êµ¬ì¡° í‚¤: {list(data.keys())}")
                
                # ë°ì´í„° ì¶”ì¶œ ë¡œì§
                items = None
                total_count = 0
                
                # ì‘ë‹µ êµ¬ì¡° í™•ì¸
                if "header" in data and "body" in data:
                    if "items" not in data["body"] or not data["body"]["items"]:
                        print(f"ğŸ“¢ í˜ì´ì§€ {page_no}ì—ì„œ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° ìˆ˜ì§‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                        break
                    
                    items = data["body"]["items"]
                    total_count = data["body"].get("totalCount", 0)
                    
                elif "response" in data and "body" in data["response"]:
                    if "items" not in data["response"]["body"] or not data["response"]["body"]["items"]:
                        print(f"ğŸ“¢ í˜ì´ì§€ {page_no}ì—ì„œ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° ìˆ˜ì§‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                        break
                    
                    items = data["response"]["body"]["items"]
                    total_count = data["response"]["body"].get("totalCount", 0)
                    
                else:
                    print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ API ì‘ë‹µ êµ¬ì¡°: {list(data.keys())}")
                    retries += 1
                    if retries < max_retries:
                        time.sleep(2)
                        continue
                    break
                
                # itemsê°€ ë‹¨ì¼ ê°ì²´ì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                if isinstance(items, dict):
                    items = [items]
                
                # ì¤‘ë³µ í™•ì¸ ë¡œì§
                if len(items) == last_page_items and last_page_items > 0:
                    # í•­ëª© ê°œìˆ˜ê°€ ì´ì „ê³¼ ê°™ì€ ê²½ìš°, ë‚´ìš©ë„ ì¤‘ë³µì¸ì§€ í™•ì¸
                    current_item_sequences = [item.get("ITEM_SEQ", "") for item in items]
                    current_sequences_set = set(current_item_sequences)
                    
                    # ì´ë¯¸ ì²˜ë¦¬í•œ ì•„ì´í…œì¸ì§€ í™•ì¸
                    if current_sequences_set.issubset(seen_item_sequences):
                        print(f"ğŸ›‘ í˜ì´ì§€ {page_no}ì˜ ëª¨ë“  í•­ëª©ì´ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤. ë°ì´í„° ìˆ˜ì§‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                        break
                
                # í•„ìš”í•œ í•„ë“œë§Œ í•„í„°ë§í•˜ê³  XML íŒŒì‹±
                processed_items = []
                new_item_count = 0
                
                for item in items:
                    item_seq = item.get("ITEM_SEQ", "")
                    
                    # ì´ë¯¸ ì²˜ë¦¬í•œ ì•„ì´í…œ ê±´ë„ˆë›°ê¸°
                    if item_seq in seen_item_sequences:
                        continue
                    
                    # í—ˆê°€ ì·¨ì†Œëœ ì˜ì•½í’ˆ í•„í„°ë§
                    if item.get("CANCEL_DATE"):
                        print(f"ğŸ“¢ í—ˆê°€ ì·¨ì†Œëœ ì˜ì•½í’ˆ ì œì™¸: {item.get('ITEM_NAME', 'ì´ë¦„ ì—†ìŒ')} (ì·¨ì†Œì¼: {item['CANCEL_DATE']})")
                        filtered_canceled_count += 1
                        continue
                    
                    # ìˆ˜ì¶œìš© ì˜ì•½í’ˆ í•„í„°ë§
                    item_name = item.get("ITEM_NAME", "")
                    if "(ìˆ˜ì¶œìš©)" in item_name:
                        print(f"ğŸ“¢ ìˆ˜ì¶œìš© ì˜ì•½í’ˆ ì œì™¸: {item_name}")
                        filtered_export_count += 1
                        continue
                    
                    seen_item_sequences.add(item_seq)
                    new_item_count += 1
                    
                    # í•„ìš”í•œ í•„ë“œë§Œ ì¶”ì¶œ
                    filtered_item = {}
                    for key in REQUIRED_FIELDS:
                        filtered_item[key] = item.get(key, "")
                    
                    # ETC_OTC_CODE ì²˜ë¦¬
                    if filtered_item.get("ETC_OTC_CODE") and not filtered_item.get("ETC_OTC_NAME"):
                        filtered_item["ETC_OTC_NAME"] = filtered_item["ETC_OTC_CODE"]
                    
                    # XML ë¬¸ì„œ íŒŒì‹± (ê°œì„ ëœ íŒŒì‹± ë¡œì§ ì‚¬ìš©)
                    for field in ['EE_DOC_DATA', 'UD_DOC_DATA', 'NB_DOC_DATA']:
                        if filtered_item.get(field):
                            try:
                                # XML íŒŒì‹± ì‹œë„
                                parsed_doc = parse_xml_doc(filtered_item[field])
                                filtered_item[f"{field}_PARSED"] = parsed_doc
                                
                                # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¡œê·¸ ì¶œë ¥
                                if parsed_doc and parsed_doc.get('type') == 'error':
                                    print(f"âš ï¸ {field} í•„ë“œ íŒŒì‹± ì‹¤íŒ¨: {parsed_doc.get('error')}")
                            except Exception as e:
                                print(f"âŒ {field} íŒŒì‹± ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
                                # ë°±ì—… ì²˜ë¦¬: í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
                                filtered_item[f"{field}_PARSED"] = extract_text_from_broken_xml(filtered_item[field])
                    
                    processed_items.append(filtered_item)

                # ìƒˆë¡œìš´ ì•„ì´í…œì´ ì—†ìœ¼ë©´ ì¢…ë£Œ
                if new_item_count == 0:
                    print(f"ğŸ“¢ í˜ì´ì§€ {page_no}ì—ì„œ ìƒˆë¡œìš´ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° ìˆ˜ì§‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                # ë°ì´í„° ì¶”ê°€
                total_data.extend(processed_items)
                print(f"âœ… í˜ì´ì§€ {page_no}ì—ì„œ {len(processed_items)}ê°œ ë ˆì½”ë“œë¥¼ ê°€ì ¸ì™€ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")
                print(f"ğŸ“Š í˜„ì¬ê¹Œì§€ í•„í„°ë§ëœ ì˜ì•½í’ˆ: í—ˆê°€ ì·¨ì†Œ {filtered_canceled_count}ê°œ, ìˆ˜ì¶œìš© {filtered_export_count}ê°œ")
                
                # ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬ í™•ì¸
                if total_count > 0:
                    estimated_pages = (total_count + page_size - 1) // page_size  # ì˜¬ë¦¼ ë‚˜ëˆ—ì…ˆ
                    print(f"ğŸ“Š í˜„ì¬ ì§„í–‰ ìƒí™©: í˜ì´ì§€ {page_no}/{estimated_pages}, ì´ ë°ì´í„°: {len(total_data)}/{total_count}")
                    
                    if page_no >= estimated_pages:
                        print(f"ğŸ“¢ ë§ˆì§€ë§‰ í˜ì´ì§€ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ë°ì´í„° ìˆ˜ì§‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                        break
                
                # í˜ì´ì§€ í¬ê¸°ë³´ë‹¤ ì ì€ ë°ì´í„°ê°€ ë°˜í™˜ë˜ë©´ ë§ˆì§€ë§‰ í˜ì´ì§€ë¡œ ê°„ì£¼
                if len(items) < page_size:
                    print(f"ğŸ“¢ í˜ì´ì§€ í¬ê¸°ë³´ë‹¤ ì ì€ í•­ëª©ì„ ë°›ì•˜ìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ í˜ì´ì§€ë¡œ ì¶”ì •ë©ë‹ˆë‹¤.")
                    break
                
                # ì„±ê³µ í‘œì‹œ
                success = True
                last_page_items = len(items)
                
            except requests.exceptions.RequestException as e:
                print(f"âŒ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
                retries += 1
                if retries < max_retries:
                    print(f"ì¬ì‹œë„ ì¤‘... ({retries}/{max_retries})")
                    time.sleep(2)
                else:
                    break
            except Exception as e:
                print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
                retries += 1
                if retries < max_retries:
                    print(f"ì¬ì‹œë„ ì¤‘... ({retries}/{max_retries})")
                    time.sleep(2)
                else:
                    print("ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ì§„í–‰ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    return total_data
        
        # ì„±ê³µí•˜ì§€ ëª»í–ˆìœ¼ë©´ ì¢…ë£Œ
        if not success:
            print(f"âš ï¸ í˜ì´ì§€ {page_no} ì²˜ë¦¬ ì‹¤íŒ¨. ë°ì´í„° ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            break
        
        # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
        page_no += 1
    
    print(f"âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ. ì´ {len(total_data)}ê°œ í•­ëª© ìˆ˜ì§‘, í•„í„°ë§ëœ í•­ëª©: í—ˆê°€ ì·¨ì†Œ {filtered_canceled_count}ê°œ, ìˆ˜ì¶œìš© {filtered_export_count}ê°œ")
    return total_data

def process_and_save_data(data, raw_file, processed_file):
    """
    ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.
    ì´ë¯¸ fetch_drug_approval_dataì—ì„œ í•„í„°ë§ì„ í–ˆì§€ë§Œ, ì•ˆì „ì„ ìœ„í•´ í•œë²ˆ ë” í•„í„°ë§í•©ë‹ˆë‹¤.
    """
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(os.path.dirname(raw_file), exist_ok=True)
    os.makedirs(os.path.dirname(processed_file), exist_ok=True)
    
    # ì›ë³¸ ë°ì´í„° ì €ì¥
    print(f"ğŸ“ ì›ë³¸ ë°ì´í„°ë¥¼ {raw_file}ì— ì €ì¥í•©ë‹ˆë‹¤...")
    with open(raw_file, "w", encoding="utf-8") as raw_json_file:
        json.dump(data, raw_json_file, ensure_ascii=False, indent=2)
    
    # í•œë²ˆ ë” í•„í„°ë§ ì ìš© (ì•ˆì „ í™•ì¸)
    filtered_data = []
    filtered_canceled_count = 0
    filtered_export_count = 0
    
    for item in data:
        # í—ˆê°€ ì·¨ì†Œëœ ì˜ì•½í’ˆ í•„í„°ë§
        if item.get("CANCEL_DATE"):
            filtered_canceled_count += 1
            continue
        
        # ìˆ˜ì¶œìš© ì˜ì•½í’ˆ í•„í„°ë§
        item_name = item.get("ITEM_NAME", "")
        if "(ìˆ˜ì¶œìš©)" in item_name:
            filtered_export_count += 1
            continue
        
        filtered_data.append(item)
    
    if filtered_canceled_count > 0 or filtered_export_count > 0:
        print(f"âš ï¸ ì¶”ê°€ í•„í„°ë§: í—ˆê°€ ì·¨ì†Œ ì˜ì•½í’ˆ {filtered_canceled_count}ê°œ, ìˆ˜ì¶œìš© ì˜ì•½í’ˆ {filtered_export_count}ê°œê°€ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•œ ë°ì´í„° ìƒì„±
    text_data = []
    for item in filtered_data:
        # ê¸°ë³¸ í•„ë“œ ì¶”ì¶œ
        text_item = {
            'ITEM_SEQ': item.get('ITEM_SEQ', ''),
            'ITEM_NAME': item.get('ITEM_NAME', ''),
            'ENTP_NAME': item.get('ENTP_NAME', ''),
            'ETC_OTC_NAME': item.get('ETC_OTC_NAME', ''),
            'CHART': item.get('CHART', ''),
            'STORAGE_METHOD': item.get('STORAGE_METHOD', '').strip(),
            'VALID_TERM': item.get('VALID_TERM', '')
        }
        
        # ê° XML ë¬¸ì„œ í•„ë“œ ì²˜ë¦¬
        for field, target_field in [
            ('EE_DOC_DATA_PARSED', 'EFFECTIVENESS'),  # íš¨ëŠ¥íš¨ê³¼
            ('UD_DOC_DATA_PARSED', 'USAGE_DOSAGE'),   # ìš©ë²•ìš©ëŸ‰
            ('NB_DOC_DATA_PARSED', 'PRECAUTIONS')     # ì‚¬ìš©ìƒì˜ì£¼ì˜ì‚¬í•­
        ]:
            # íŒŒì‹±ëœ ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if field in item and item[field]:
                # íŒŒì‹±ëœ ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ í•„ë“œ ì¶”ì¶œ
                parsed_text = item[field].get('text', '')
                text_item[target_field] = parsed_text
            else:
                # íŒŒì‹±ëœ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ì›ë³¸ XMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
                original_field = field.replace('_PARSED', '')
                if original_field in item and item[original_field]:
                    try:
                        extracted_text = extract_text_from_broken_xml(item[original_field]).get('text', '')
                        text_item[target_field] = extracted_text
                    except Exception as e:
                        print(f"âŒ {original_field} í•„ë“œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
                        text_item[target_field] = f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)[:100]}"
                else:
                    text_item[target_field] = ''
        
        # ë°ì´í„° í’ˆì§ˆ í™•ì¸
        has_content = False
        for field in ['EFFECTIVENESS', 'USAGE_DOSAGE', 'PRECAUTIONS']:
            if text_item.get(field) and len(text_item[field]) > 10:  # ì˜ë¯¸ ìˆëŠ” ë‚´ìš©ì´ ìˆëŠ”ì§€ í™•ì¸
                has_content = True
                break
        
        # ì˜ë¯¸ ìˆëŠ” ë‚´ìš©ì´ ìˆëŠ” í•­ëª©ë§Œ ì¶”ê°€
        if has_content or (text_item.get('ITEM_NAME') and text_item.get('ENTP_NAME')):
            text_data.append(text_item)
        else:
            print(f"âš ï¸ í•­ëª© '{text_item.get('ITEM_NAME', 'ì´ë¦„ ì—†ìŒ')}' (ID: {text_item.get('ITEM_SEQ', 'ì¼ë ¨ë²ˆí˜¸ ì—†ìŒ')})ì— ì˜ë¯¸ ìˆëŠ” ë‚´ìš©ì´ ì—†ì–´ ì œì™¸í•©ë‹ˆë‹¤.")
    
    # ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
    print(f"ğŸ“ ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ {processed_file}ì— ì €ì¥í•©ë‹ˆë‹¤...")
    with open(processed_file, "w", encoding="utf-8") as processed_json_file:
        json.dump(text_data, processed_json_file, ensure_ascii=False, indent=2)
    
    print(f"âœ… ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ. ì›ë³¸ ë ˆì½”ë“œ ìˆ˜: {len(data)}, í•„í„°ë§ í›„ ë ˆì½”ë“œ ìˆ˜: {len(filtered_data)}, ìµœì¢… ìœ íš¨ ë ˆì½”ë“œ ìˆ˜: {len(text_data)}")
    
    # ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥
    if text_data:
        print("\n[ì²˜ë¦¬ëœ ë°ì´í„° ìƒ˜í”Œ]")
        sample = text_data[0]
        print(f"ì˜ì•½í’ˆëª…: {sample.get('ITEM_NAME', 'ì´ë¦„ ì—†ìŒ')}")
        print(f"ì—…ì²´ëª…: {sample.get('ENTP_NAME', 'ì—…ì²´ëª… ì—†ìŒ')}")
        print(f"ì„±ìƒ: {sample.get('CHART', 'ì„±ìƒ ì •ë³´ ì—†ìŒ')}")
        print(f"ë¶„ë¥˜: {sample.get('ETC_OTC_NAME', 'ë¶„ë¥˜ ì •ë³´ ì—†ìŒ')}")
        
        # íš¨ëŠ¥íš¨ê³¼, ìš©ë²•ìš©ëŸ‰, ì£¼ì˜ì‚¬í•­ ë¯¸ë¦¬ë³´ê¸° ì¶œë ¥
        for field, label in [
            ('EFFECTIVENESS', 'íš¨ëŠ¥íš¨ê³¼'), 
            ('USAGE_DOSAGE', 'ìš©ë²•ìš©ëŸ‰'), 
            ('PRECAUTIONS', 'ì£¼ì˜ì‚¬í•­')
        ]:
            preview = sample.get(field, '')
            if preview:
                # ê¸´ ë‚´ìš©ì€ ì¶•ì•½í•´ì„œ ë³´ì—¬ì¤Œ
                preview_text = preview[:100] + ('...' if len(preview) > 100 else '')
                print(f"{label} ë¯¸ë¦¬ë³´ê¸°: {preview_text}")
            else:
                print(f"{label}: ì •ë³´ ì—†ìŒ")
    else:
        print("âš ï¸ ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")

    # ì—ëŸ¬ ë³´ê³ ì„œ ìƒì„± (ì„ íƒì )
    error_records = []
    for item in filtered_data:
        error_fields = []
        
        # XML íŒŒì‹± ì˜¤ë¥˜ í™•ì¸
        for field in ['EE_DOC_DATA_PARSED', 'UD_DOC_DATA_PARSED', 'NB_DOC_DATA_PARSED']:
            if field in item and item[field] and item[field].get('type') == 'error':
                error_fields.append({
                    'field': field.replace('_PARSED', ''),
                    'error': item[field].get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')
                })
        
        # ì˜¤ë¥˜ê°€ ìˆëŠ” ë ˆì½”ë“œ ì •ë³´ ì¶”ê°€
        if error_fields:
            error_records.append({
                'ITEM_SEQ': item.get('ITEM_SEQ', ''),
                'ITEM_NAME': item.get('ITEM_NAME', ''),
                'error_fields': error_fields
            })
    
    # ì˜¤ë¥˜ ë³´ê³ ì„œ ì¶œë ¥
    if error_records:
        error_report_file = os.path.join(os.path.dirname(processed_file), "error_report.json")
        with open(error_report_file, "w", encoding="utf-8") as error_file:
            json.dump(error_records, error_file, ensure_ascii=False, indent=2)
        print(f"âš ï¸ {len(error_records)}ê°œì˜ ë ˆì½”ë“œì—ì„œ XML íŒŒì‹± ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ {error_report_file}ì„ ì°¸ì¡°í•˜ì„¸ìš”.")

if __name__ == "__main__":
    print("ğŸ” ì˜ì•½í’ˆ í—ˆê°€ ì •ë³´ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
    drug_data = fetch_drug_approval_data()
    
    if drug_data:
        print(f"âœ… {len(drug_data)}ê°œì˜ ì˜ì•½í’ˆ ë ˆì½”ë“œë¥¼ ì„±ê³µì ìœ¼ë¡œ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
        process_and_save_data(drug_data, RAW_OUTPUT_FILE, PROCESSED_OUTPUT_FILE)
    else:
        print("âŒ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. API ì—°ê²° ë° ë§¤ê°œë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")